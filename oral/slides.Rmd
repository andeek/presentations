---
title: "On advancing MCMC-based methods for Markovian data structures with applications to deep learning, simulation, and resampling"
shorttitle: "MCMC-based methods for Markovian data"
author: "Andee Kaplan"
date: "October 25, 2016"
output: 
  beamer_presentation:
    template: beamer.tex
    citation_package: natbib
    includes:
      in_header: front-matter.tex
theme: CambridgeUS
bibliography: refs.bib
fig_caption: true
---

---- 

```{r libraries, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(ggplot2)
library(dplyr)
library(tidyr)

opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)
theme_set(theme_bw())

source("helpers/functions.R")
```

\begin{center}
\Huge
An exposition on the propriety of restricted Boltzmann machines
\end{center}

# What is this?

A Restricted Boltzman Machine (RBM) is an undirected probabilistic graphical model (for discrete or continuous random variables) with two layers, one hidden and one visible, with conditional independence within a layer [@smolensky1986information].

\begin{figure}
\includegraphics[width=.6\linewidth]{images/rbm.png}
\caption{An example RBM, which consists of two layers. Hidden nodes are indicated by white circles and the visible nodes are indicated by blue circles}
\end{figure}

# How is it used?

Typically used for image classification. Each image pixel is a node in the visible layer. The output creates features, which are passed to a supervised learning algorithm.

\begin{figure}
\includegraphics[width=\linewidth]{images/visibles_image.png}
\caption{Image classification using a RBM. On the left, each image pixel comprises a node in the visible layer, $\mathcal{V}$. On the right, the output of the RBM is used to create features which are then passed to a supervised learning algorithm.}
\end{figure}

# Joint distribution
Let $\boldsymbol x = {h_1, \dots, h_H, v_1,\dots,v_V}$ represent the states of the visible and hidden nodes in an RBM. We will consider both $\boldsymbol x \in \{0,1\}^{H + V}$ and $\boldsymbol x \in \{-1, 1\}^{H + V}$. 

A parametric form for probabilities corresponding to a potential vector of states of each node taking the value of $1$.

\begin{align} 
\label{eqn:pmf} 
f_{\boldsymbol \theta} (\boldsymbol x) = \frac{\exp\left(\sum\limits_{i = 1}^V \sum\limits_{j=1}^H \theta_{ij} v_i h_j + \sum\limits_{i = 1}^V\theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j\right)}{\gamma(\boldsymbol \theta)} \end{align}

Where 

$$\gamma(\boldsymbol \theta) = \sum\limits_{\boldsymbol x \in \mathcal{X}}\exp\left(\sum\limits_{i = 1}^V \sum\limits_{j=1}^H \theta_{ij} v_i h_j + \sum\limits_{i = 1}^V\theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j\right)$$.

# Deep learning

\begin{columns}[T] % align columns
\begin{column}{.48\textwidth}
\begin{itemize}
\item By stacking layers of RBMs in a deep architecture, proponents of the models claim the ability to learn "internal representations that become increasingly complex, which is considered to be a promising way of solving object and speech recognition problems" \cite[pp. 450]{salakhutdinov2009deep}.
\item Stacking is achieved by treating a hidden layer of one RBM as the visible layer in a second RBM, etc.
\end{itemize}
\end{column}
\hfill
\begin{column}{.48\textwidth}
\begin{figure}
\includegraphics[width=\linewidth]{images/deep_rbm.png}
\caption{Three layer deep Boltzmann machine, with visible-to-hidden and hidden-to-hidden connections but no within-layer connections.}
\end{figure}
\end{column}
\end{columns}

# Why do I care?

- The model properties are largely unexplored in the literature and the commonly cited fitting methodology remains heuristic-based and abstruse. 
- We want to 
    1. provide steps toward a thorough understanding of the model and its behavior from the perspective of statistical model theory, and 
    2. explore the possibility of a rigorous fitting methodology. 

# Degeneracy, instability, and uninterpretability. Oh my!

The highly flexible nature of a RBM (having as it does $H + V + H V$ parameters) makes three kinds of potential model impropriety of concern, *degeneracy*, *instability*, and *uninterpretability*.

> A model should "provide an explanation of the mechanism underlying the observed phenomena" [@lehmann1990model, @box1967discrimination]. 

RBMs often fail to generate data that resemble realistic data and thus an unsatisfactory conceptualization of the data generation process. Additionally, we find that RBMs easily exhibit a kind of instability in the parameter space. In practice, this is seen when a single pixel change in an image results in a wildly different classification. Such model impropriety issues have been documented in other deep architectures recently [@szegedy2013intriguing, @nguyen2014deep].

# Near-degeneracy

\begin{definition}[Model Degeneracy]
There is a disproportionate amount of probability placed on only a few elements of the sample space, $\mathcal{X}$, by the model.
\end{definition}

The RBM class of models exhibits *near-degeneracy* when random variables in the neg-potential function 
$$Q(\boldsymbol x) = \sum\limits_{i = 1}^V \sum\limits_{j=1}^H \theta_{ij} v_i h_j + \sum\limits_{i = 1}^V\theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j,
$$ 
have a mean vector $\mu(\boldsymbol \theta)$ close to the boundary of the convex hull of $\mathcal{T}$ [@handcock2003assessing], where $t(\boldsymbol x) = \{v_1, \dots, v_V, h_1, \dots, h_H, v_1 h_1, \dots, v_V h_H \} \in \mathcal{T}$ and the mean parameterization on the model parameters, $\mu(\boldsymbol \theta)$ is

---

\begin{align*}
\mu(\boldsymbol \theta) &= \text{E}_{\boldsymbol \theta} t(\boldsymbol X)  \\
&= \sum\limits_{x \in \mathcal{X}} \left\{ t(\boldsymbol x) f_{\boldsymbol \theta} (\boldsymbol x) \right\} \\
&= \sum\limits_{x \in \mathcal{X}} \left\{ t(\boldsymbol x)\dfrac{\exp\left(\sum\limits_{i = 1}^V \sum\limits_{j=1}^H \theta_{ij} v_i h_j + \sum\limits_{i = 1}^V\theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j\right)}{\sum\limits_{\boldsymbol x \in \mathcal{X}}\exp\left(\sum\limits_{i = 1}^V \sum\limits_{j=1}^H \theta_{ij} v_i h_j + \sum\limits_{i = 1}^V\theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j\right)}\right\}. \\
\end{align*}

# Instability

\begin{definition}[Instability]
Characterized by excessive sensitivity in the model, where small changes in the components of potential data outcomes, $\boldsymbol x$, may lead to substantial changes in the probability mass function.
\end{definition}

@schweinberger2011instability introduced a concept of model defficiency related to *instability* considering only a class of exponential families of distributions.

To quantify *instability* in the RBM, it is useful to imagine how a data model might be expanded to incorporate more and more observations. To increase the size of a RBM model to handle more "visible variables", it becomes necessary to expand the number of model parameters (and in the process of increasing the model size in visibles, one may also arbitrarily expand the number of hidden variables to be included).

# Unstable RBMs

\begin{definition}[Unstable RBM]
A RBM model formulation is \emph{unstable} if, as the number of visible variables increase ($V \rightarrow \infty$), it holds that 
\begin{align*}
\lim\limits_{V \rightarrow \infty} \frac{1}{V} ELPR_V(\boldsymbol \theta) = \infty.
\end{align*}
where 
\begin{align}
\label{eqn:elpr}
ELPR_V(\boldsymbol \theta) = \log \left[\frac{\max\limits_{(v_1, \dots, v_V) \in \{-1,1\}^V}P_{\boldsymbol \theta}(v_1, \dots, v_V)}{\min\limits_{(v_1, \dots, v_V) \in \{-1,1\}^V}P_{\boldsymbol \theta}(v_1, \dots, v_V)}\right]
\end{align}
\end{definition}

Unstable RBM models are undesirable for several reasons. One is that, as mentioned above, small changes in data can lead to overly-sensitive changes in probability. 

# One-pixel change

Consider, for example, the biggest log-probability ratio for a one-pixel (one component) change in data outcomes (visibles). 

$$
\Delta_V(\boldsymbol \theta) \equiv \max \left\{\log \frac{P_{\boldsymbol \theta}(v_1, \dots, v_V)}{P_{\boldsymbol \theta}(v_1^*, \dots, v_V^*)} \right\},
$$

where $(v_1, \dots, v_V) \& (v_1^*, \dots, v_V^*) \in \{-1,1\}^V$ differ by exactly one component


\begin{result}
\label{prop:instab}
If $\frac{1}{V}\text{ELPR}_V(\boldsymbol \theta) > C$, then $\Delta_V(\boldsymbol \theta) > C$.
\end{result}

In other words, if the quantity (\ref{eqn:elpr}) is too large, then the RBM model will exhibit large probability shifts for very small changes in the data configuration (i.e. instability mentioned earlier).

# Tie to degeneracy

Unstable RBM models are connected to degenerate models (placing all probability on a small pieace of the sample space). Define a model set 
\begin{align*}
M_{\epsilon, \boldsymbol \theta} &\equiv \left\{\boldsymbol v \in \{-1,1\}^V: \log P_{\boldsymbol \theta}(\boldsymbol v)  > (1-\epsilon)\max\limits_{\boldsymbol v^*}P_{\boldsymbol \theta}(\boldsymbol v^*) + \epsilon\min\limits_{\boldsymbol v^*}P_{\boldsymbol \theta}(\boldsymbol v^*) \right\}
\end{align*}

of possible outcomes, for a given $0 < \epsilon < 1$.

\begin{result}
\label{prop:degen}
For an unstable RBM model, and for any given $0 < \epsilon < 1$, $P_{\boldsymbol \theta}\left((v_1, \dots, v_V) \in M_{\epsilon, \boldsymbol \theta}\right) \rightarrow 1$ holds as $V \rightarrow \infty$.
\end{result}

In other words, as a consequence of unstable models, all probability will stack up on mode sets or potentially those few outcomes with the highest probability. Proofs of results \ref{prop:instab}-\ref{prop:degen} are provided in the appendix.

# Computationally tractable lower bound

To numerically investigate unstable RBM models, it is useful to define a computationally tractable lower bound on (\ref{eqn:elpr}) given by
$$
\frac{1}{V}R(\boldsymbol \theta) \text{ for } R(\boldsymbol \theta) \equiv \max_{\boldsymbol v} \max_{\boldsymbol h}Q(\boldsymbol x) - \min_{\boldsymbol v}\max_{\boldsymbol h}Q(\boldsymbol x) - H\log 2.
$$
That is, it holds that, in (\ref{eqn:elpr}),
\begin{align}
\frac{1}{V} \text{ELPR}_V(\boldsymbol \theta) \ge \frac{1}{V}R(\boldsymbol \theta). \label{eqn:lowerbnd}
\end{align}
(A proof of (\ref{eqn:lowerbnd}) appears in the appendix.)

# Uninterpretability

\begin{definition}[Uninterpretability]
Characterized by marginal mean-structure not being maintained in the model due to dependence \cite{kaiser2007statistical}.
\end{definition}

A measure of this is the magnitude of the difference between model expectations, E$\left[\boldsymbol X | \boldsymbol \theta\right]$, and expectations given independence, E$\left[\boldsymbol X | \boldsymbol \theta^* \right]$, where $\boldsymbol \theta^*$ is defined to equal $\boldsymbol \theta$ with all $\theta_{ij} = 0$ for $i = 1, \dots, V, j = 1, \dots, H$. 

Using this concept it is possible to investigate what conditions lead to uninterpretability in a model versus those that guarantee interpretable models. A model is considered without uninterpretability if its expected value is not very different from the same model with assumed independence. 

# RBM quantities to compare

\begin{align*}
\text{E}\left[\boldsymbol X | \boldsymbol \theta\right] &= \sum\limits_{\boldsymbol x \in \mathcal{X}} \boldsymbol x f_{\boldsymbol \theta}(\boldsymbol x) \\
&= \sum\limits_{\boldsymbol x \in \mathcal{X}} \boldsymbol x \frac{\exp\left(\sum\limits_{i = 1}^V \sum\limits_{j=1}^H \theta_{ij} v_i h_j + \sum\limits_{i = 1}^V\theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j\right)}{\sum\limits_{\boldsymbol x \in \mathcal{X}}\exp\left(\sum\limits_{i = 1}^V \sum\limits_{j=1}^H \theta_{ij} v_i h_j + \sum\limits_{i = 1}^V\theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j\right)} \\
&= \frac{\sum\limits_{\boldsymbol x \in \mathcal{X}} x_k \exp\left(\sum\limits_{i = 1}^V \sum\limits_{j=1}^H \theta_{ij} v_i h_j + \sum\limits_{i = 1}^V\theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j\right)}{\sum\limits_{\boldsymbol x \in \mathcal{X}}\exp\left(\sum\limits_{i = 1}^V \sum\limits_{j=1}^H \theta_{ij} v_i h_j + \sum\limits_{i = 1}^V\theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j\right)}
\end{align*}
for $k = 1, \dots, H + V$.

# RBM quantities to compare

\begin{align*}
\text{E}\left[\boldsymbol X | \boldsymbol 
\theta^*\right] &= \sum\limits_{\boldsymbol x \in \mathcal{X}} \boldsymbol x \frac{\exp\left(\sum\limits_{i = 1}^V \theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j\right)}{\sum\limits_{\boldsymbol x \in \mathcal{X}}\exp\left(\sum\limits_{i = 1}^V\theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j\right)} \\
&= \frac{\sum\limits_{\boldsymbol x \in \mathcal{X}} x_k \exp\left(\sum\limits_{i = 1}^V\theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j\right)}{\sum\limits_{\boldsymbol x \in \mathcal{X}}\exp\left(\sum\limits_{i = 1}^V\theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j\right)}
\end{align*}
for $k = 1, \dots, H + V$.

If $|\text{E}\left[\boldsymbol X | \boldsymbol \theta\right] - \text{E}\left[\boldsymbol X | \boldsymbol \theta^*\right]|$ is large then the RBM with parameter vector $\boldsymbol \theta$ is *uninterpetable*.

# Data coding to mitigate degeneracy

\begin{figure}
\includegraphics[width=.49\linewidth]{images/toyhull_top.png}
\includegraphics[width=.49\linewidth]{images/toyhull_top_2.png}
\caption{The convex hull of the "statistic space" in three dimensions for the toy RBM with one visible and one hidden node for $\{0,1\}$ (left) and $\{-1,1\}$ (right) data encoding. The convex hull of $\mathcal{T}$ does not fill the unit cube because of the relationship between statistics.}
\end{figure}

# The center of the universe

- For the $\{-1, 1 \}$ encoding of $\mathcal{V}$ and $\mathcal{H}$, the origin is the center of the parameter space
- At $\theta = 0$, RBM is equivalent to elements of $X$ being distributed as iid Bernoulli$\left(\frac{1}{2}\right)$ $\Rightarrow$ No *near-degeneracy*, *instability*, or *uninterpretability*!


```{r encoding-volume, echo = FALSE, fig.height=3, fig.cap="The relationship between volume of the convex hull of $\\mathcal{T}$ and the cube containing it for different configurations of nodes."}
test_cases <- expand.grid(data.frame(1:4)[,rep(1,2)]) %>% 
  rename(H = X1.4, V = X1.4.1) %>%
  filter(H <= V) %>% #remove those cases with less visibles than hiddens. Is this necessary?
  mutate(n_param = H*V + H + V) %>%
  mutate(max_facets = (2^(H+V))^(floor(n_param/2))) %>%
  filter(n_param <= 11) #calc_hull can't handle any higher dimensions currently

bin_res <- plyr::dlply(test_cases, plyr::.(n_param), function(x) calc_hull(x$V, x$H, "binary"))
neg_res <- plyr::dlply(test_cases, plyr::.(n_param), function(x) calc_hull(x$V, x$H, "negative"))

plyr::ldply(bin_res, function(x) x$c_hull$vol) %>% 
  mutate(frac_vol = V1/(1^n_param)) %>%
  inner_join(plyr::ldply(neg_res, function(x) x$c_hull$vol) %>% 
               mutate(frac_vol = V1/(2^n_param)),
             by="n_param") %>%
  rename(vol.bin = V1.x, vol.neg = V1.y, frac_vol.bin = frac_vol.x, frac_vol.neg = frac_vol.y) %>%
  gather(vars, value, -n_param) %>%
  separate(vars, c("type", "encoding"), "\\.") %>%
  spread(type, value) %>%
  ggplot() +
  geom_point(aes(x=n_param, y=frac_vol, colour=encoding)) +
  geom_line(aes(x=n_param, y=frac_vol, colour=encoding, group=encoding)) +
  ylab("Fraction of unrestricted volume") +
  xlab("Number of parameters") +
  scale_colour_discrete("Encoding", labels=c("Binary (1,0)", "Negative (1,-1)")) +           
  theme(
    plot.background = element_rect(fill = "transparent", colour = NA),
    legend.background = element_rect(fill = "transparent", colour = NA)) -> p

p
```

# Manageable (a.k.a. small) examples

```{r degen-data, message=FALSE, warning=FALSE, cache=TRUE}
#reshape data functions
plot_data <- function(res, grid = FALSE) {
  
  plot.data <- data.frame()
  
  if(!grid) {
    for(i in 1:nrow(res)) {
      tmp <- res$g_theta[[i]] %>% data.frame()
      H <- res[i,]$H
      V <- res[i,]$V
      N <- res[i,]$N
      r1 <- res[i,]$r1
      r2 <- res[i,]$r2
      C <- res[i,]$C
      epsilon <- res[i,]$epsilon
      
      
      tmp %>% 
        rowwise() %>% 
        mutate_(ss_interaction = paste0("(", paste(paste0(names(.)[(H + V + 1):(H + V + H*V)], "^2"), collapse = " + "), ")"),
                ss_main = paste0("sum(", paste(paste0(names(.)[1:(H+V)],"^2"), collapse = " + "), ")")) %>%    
        ungroup() -> ratio
      
      
      inner_join(ratio, res$outside[[i]] %>% ungroup()) %>%
        select(ss_interaction, ss_main, near_hull) %>%
        mutate(H = H, V = V, n_param = H + V + H*V, N = H + V, N = N, r1 = r1, r2 = r2, C = C, epsilon = epsilon) %>%
        rbind(plot.data) -> plot.data
    }
  } else {
    for(i in 1:nrow(res)) {
      tmp <- res$g_theta[[i]] %>% data.frame()
      H <- res[i,]$H
      V <- res[i,]$V
      N <- res[i,]$N
      r1 <- res[i,]$r1
      r2 <- res[i,]$r2
      
      tmp %>% 
        rowwise() %>% 
        mutate_(ss_interaction = paste0("(", paste(paste0(names(.)[(H + V + 1):(H + V + H*V)], "^2"), collapse = " + "), ")"),
                ss_main = paste0("sum(", paste(paste0(names(.)[1:(H+V)],"^2"), collapse = " + "), ")")) %>%  
        ungroup() -> ratio
      
      
      inner_join(ratio, res$outside[[i]] %>% ungroup()) %>%
        select(ss_interaction, ss_main, near_hull) %>%
        mutate(H = H, V = V, n_param = H + V + H*V, N = H + V, N = N, r1 = r1, r2 = r2) %>%
        rbind(plot.data) -> plot.data
    }
  }
  
  return(plot.data)
}
indep_params <- function(samp, H, V) {
  samp[, (H + V + 1):ncol(samp)] <- 0
  samp
}
max_Q <- function(theta, stats) {
  apply(crossprod(t(stats), theta), 2, max)
}
min_Q <- function(theta, stats) {
  apply(crossprod(t(stats), theta), 2, min)
}
min_max_Q <- function(theta, stats) {
  require(dplyr)
  tcrossprod(stats, theta) %>% data.frame() %>%
    cbind(stats) %>%
    group_by_(.dots = colnames(stats)[grepl("v", colnames(stats)) & !grepl("theta", colnames(stats))]) %>%
    summarise_each(funs(max), contains("X")) %>%
    ungroup() %>%
    summarise_each(funs(min), contains("X")) %>%
    select(contains("X")) %>%
    data.matrix() %>%
    t()
}


#grid data
load("data/results_grid.RData")

#near-degeneracy
plot_dat_grid <- res %>% plot_data(grid = TRUE)

plot_dat_grid %>%
  group_by(r1, r2, H, V) %>%
  summarise(frac_degen = sum(near_hull)/n(), count = n()) %>% 
  ungroup() %>% 
  mutate(Hiddens = paste0("H = ", H), Visibles = paste0("V = ", V)) -> convex_hull_summary

#uninterpretability
res %>%
  group_by(H, V, N, r1, r2) %>%
  do(indep_exp = t(expected_value(t(indep_params(.$samp[[1]], .$H, .$V)), .$stat[[1]]))[, -((.$H + .$V + 1):(.$H + .$V + .$H*.$V))],
     marg_exp = .$g_theta[[1]][, (.$H + .$V + .$H*.$V + 1):(ncol(.$g_theta[[1]])-.$H*.$V)]) -> exp_vals

exp_vals %>%
  group_by(H, V, N, r1, r2) %>%
  do(data.frame(max_abs_diff = apply(abs(.$indep_exp[[1]] - .$marg_exp[[1]]), 1, max))) %>%
  group_by(H, V, N, r1, r2) %>%
  summarise(max_abs_diff = mean(max_abs_diff)) %>%
  mutate(Hiddens = paste0("H = ", H), Visibles = paste0("V = ", V)) -> exp_vals_summary

#instability
res %>%
  group_by(H, V, N, r1, r2) %>%
  do(data.frame(max = max_Q(t(.$samp[[1]]), .$stat[[1]]),
                min = min_Q(t(.$samp[[1]]), .$stat[[1]]),
                min_max = min_max_Q(.$samp[[1]], .$stat[[1]]))) %>%
  ungroup() %>% 
  group_by(H, V, N, r1, r2) %>%
  mutate(LHS1 = (max - min)/N,
         LHS2 = (max - min_max - H*log(2))/N) %>%
  summarise_each(funs(mean), LHS1, LHS2) -> max_q_summary


convex_hull_summary %>%
  left_join(max_q_summary) %>%
  left_join(exp_vals_summary) -> three_ways
  

```

- To explore the behavior of the RBM parameters $\boldsymbol \theta$ as it relates to near-degeneracy, instability, and uninterpretability, we consider models of small size.  For $H, V \in \{1, \dots, 4\}$, we sample `r dim(res$samp[[1]])[1]` values of $\boldsymbol \theta$.
    1. Split $\boldsymbol \theta$ into $\boldsymbol \theta_{interaction}$ and $\boldsymbol \theta_{main}$, in reference to which sufficient statistics the parameters correspond to.
    2. Allow the two types of terms to have varying average magnitudes, $||\boldsymbol \theta_{main} || /(H+V)$ and $||\boldsymbol \theta_{interaction} || /(H*V)$.
    3. Average magnitudes vary on a grid between `r min(plot_dat_grid$r1/(plot_dat_grid$H + plot_dat_grid$V))` and `r max(plot_dat_grid$r1/(plot_dat_grid$H + plot_dat_grid$V))` with `r length(unique(plot_dat_grid$r1/(plot_dat_grid$H + plot_dat_grid$V)))` breaks, yielding `r length(unique(plot_dat_grid$r1/(plot_dat_grid$H + plot_dat_grid$V)))^2` grid points.
- Calculate the three metrics of model impropriety, $\boldsymbol \mu(\boldsymbol \theta)$, $R(\boldsymbol x)/V$, and the coordinates of $\left|\text{E}\left[\boldsymbol X | \boldsymbol \theta\right] -   \text{E}\left[\boldsymbol X | \boldsymbol \theta^* \right] \right|$. 
- In the case of *near-degeneracy*, we can go further and classify each model as near-degenerate or "viable" based on the distance of $\boldsymbol \mu(\boldsymbol \theta)$ from the boundary of the convex hull of $\mathcal{T}$
 
# Simulation results
```{r degen_plots}
three_ways %>%
  mutate(r1 = round(r1/(H + V), 8), r2 = round(r2/(H * V), 8)) %>% 
  ggplot() +
  geom_tile(aes(x = r1, y = r2, fill = frac_degen)) +
  geom_contour(aes(x = r1, y = r2, z = frac_degen), colour = "black", bins = 8) +
  geom_contour(aes(x = r1, y = r2, z = frac_degen), colour = "black", breaks = .05, size = 1.5) +
  geom_abline(aes(intercept = 0, slope = 1), alpha = .5, lty = 2) +
  scale_fill_gradient("Fraction near-\ndegenerate", low = "yellow", high = "red") +
  facet_grid(Visibles~Hiddens) +
  xlab(expression(paste(paste("||", theta[main], "||", "/(H + V)")))) +
  ylab(expression(paste(paste("||", theta[interaction], "||", "/(H*V)")))) +
  theme(aspect.ratio = 1, legend.position = "bottom") -> p.degen

three_ways %>%
  mutate(r1 = round(r1/(H + V), 8), r2 = round(r2/(H * V), 8)) %>% 
  ggplot() +
  geom_tile(aes(x = r1, y = r2, fill = max_abs_diff)) +
  geom_contour(aes(x = r1, y = r2, z = max_abs_diff), colour = "black", bins = 8) +
  #geom_contour(aes(x = r1, y = r2, z = mean_abs_diff), colour = "black", breaks = .05, size = 1.5) +
  geom_abline(aes(intercept = 0, slope = 1), alpha = .5, lty = 2) +
  scale_fill_gradient("Mean max \n abs. difference", low = "yellow", high = "red", limits = c(0,2)) +
  facet_grid(Visibles~Hiddens) +
  xlab(expression(paste(paste("||", theta[main], "||", "/(H + V)")))) +
  ylab(expression(paste(paste("||", theta[interaction], "||", "/(H*V)")))) +
  theme(aspect.ratio = 1, legend.position = "bottom") -> p.exp_diff

three_ways %>%
  mutate(r1 = round(r1/(H + V), 8), r2 = round(r2/(H * V), 8)) %>% 
  ggplot() +
  geom_tile(aes(x = r1, y = r2, fill = LHS1)) +
  geom_contour(aes(x = r1, y = r2, z = LHS1), colour = "black", bins = 8) +
  #geom_contour(aes(x = r1, y = r2, z = mean_max_q), colour = "black", breaks = .05, size = 1.5) +
  geom_abline(aes(intercept = 0, slope = 1), alpha = .5, lty = 2) +
  scale_fill_gradient(expression(frac(max(Q) - min(Q), H + V)), low = "yellow", high = "red") +
  facet_grid(Visibles~Hiddens) +
  xlab(expression(paste(paste("||", theta[main], "||", "/(H + V)")))) +
  ylab(expression(paste(paste("||", theta[interaction], "||", "/(H*V)")))) +
  theme(aspect.ratio = 1, legend.position = "bottom") -> p.max_q


three_ways %>%
  mutate(r1 = round(r1/(H + V), 8), r2 = round(r2/(H * V), 8)) %>% 
  ggplot() +
  geom_tile(aes(x = r1, y = r2, fill = LHS2)) +
  geom_contour(aes(x = r1, y = r2, z = LHS2), colour = "black", bins = 8) +
  #geom_contour(aes(x = r1, y = r2, z = mean_max_q), colour = "black", breaks = .05, size = 1.5) +
  geom_abline(aes(intercept = 0, slope = 1), alpha = .5, lty = 2) +
  scale_fill_gradient(expression(frac(bar(R(theta)), V)), low = "yellow", high = "red") +
  facet_grid(Visibles~Hiddens) +
  xlab(expression(paste(paste("||", theta[main], "||", "/(H + V)")))) +
  ylab(expression(paste(paste("||", theta[interaction], "||", "/(H*V)")))) +
  theme(aspect.ratio = 1, legend.position = "bottom") -> p.min_max_q

```


```{r, fig.show="hold", fig.width=3, out.width='.32\\linewidth', fig.cap="\\label{fig:degen_plots}Results from the numerical experiment, here looking at the fraction of models that were near-degenerate (left), the sample mean value of $R(\\boldsymbol \\theta)/V$ (middle), and the sample mean of the maximum component of the absolute difference between the model expectation vector, E$\\left[\\boldsymbol X | \\boldsymbol \\theta\\right]$, and the expectation vector given independence, E$\\left[\\boldsymbol X | \\boldsymbol \\theta^* \\right ]$ (right)."}
p.degen
p.min_max_q
p.exp_diff
```

# Model fitting

- Typically, fitting a RBM via maximum likelihood (ML) methods will be infeasible mainly due to the intractibility of the normalizing term $\gamma(\boldsymbol \theta)$ in a model of any realistic size
    - Ad hoc methods are used instead, which aim to avoid this problem by using stochastic ML that employ a small number of MCMC draws. 
- Computational concerns are not the only issues with fitting an RBM using ML, the RBM model, has the potential to re-create any distribution for the data.
    - Based on a random sample of visible variables, the model for the cell probabilities that has the highest likelihood over  *all possible model classes* is the empirical distribution, and the parametrization of the RBM model itself ensures that this empirical distribution can be arbitrarily well approximated.
    - Whenever the empirical distribution contains empty cells, fitting steps for the RBM model will aim to chase parameters that necessarily diverge in magnitude in order to zero out the corresponding RBM cell probabilities.

# Bayesian methods

- We consider what might be done in a principled manner, testing on a $V = H = 4$ case that already stretched the limits of what is computable - in particular we consider Bayes methods. 
- To avoid model impropriety for a fitted RBM, we want to avoid parts of the parameter space $\mathbb{R}^{V + H + VH}$ that lead to *near-degeneracy*, *instability*, and *uninterpretability*.
    - Shrink $\boldsymbol \theta$ toward $\boldsymbol 0$ by specifying priors that place low probability on large values of $||\boldsymbol \theta||$, shrinking $\boldsymbol \theta_{interaction}$ more than $\boldsymbol \theta_{main}$.
  - We considered a test case with $V = H = 4$ and parameters given in in appendix. This parameter vector was chosen as a sampled value of $\boldsymbol \theta$ that was not near the convex hull of the sufficient statistics for a grid point in figure \ref{fig:degen_plots} with $< 5$\% near-degeneracy. We simulated $n = 5,000$ as a training set and fit the RBM using three Bayes methodologies. 
  
# Fitting methodologies  

1. *A trick prior.* Here we cancel out normalizing term in the likelihood so that resulting full conditionals of $\boldsymbol \theta$ are multivariate Normal. The $h_j$ are carried along as latent variables.
    \begin{align*}
    \pi(\boldsymbol \theta) \propto \gamma(\boldsymbol \theta)^n \exp\left(-\frac{1}{2C_{1}}\boldsymbol \theta_{main}'\boldsymbol \theta_{main} -\frac{1}{2C_{2}}\boldsymbol \theta_{interaction}'\boldsymbol \theta_{interaction}\right), \vspace{-.75cm}
    \end{align*}
    where $C_{2} < C_{1}$. This is the method of @li2014biclustering.
2. *A truncated Normal prior.* Use independent truncated spherical normal distributions as priors for $\boldsymbol \theta_{main}$ and $\boldsymbol \theta_{interaction}$ with $\sigma_{interaction} < \sigma_{main}$. Full conditionals are not conjugate, and simulaion from the posterior was accomplished using a geometric adaptive Metropolis Hastings step [@zhou2014some] and calculation of likelihood normalizing constant. Here the $h_j$ are carried along as latent variables.
3. *A truncated Normal prior and marginalized likelihood.* Marginalize out $\boldsymbol h$ in $f_{\boldsymbol \theta}(\boldsymbol x)$, and use the truncated Normal prior. 

# Hyperparameters

\begin{table}[ht]
\centering
\caption{The values used for the hyperparameters for all three fitting methods. A rule of thumb is imposed which decreases prior variances for the model parameters as the size of the model increases and also shrinks $\boldsymbol \theta_{interaction}$ more than $\boldsymbol \theta_{main}$.}
\label{tab:hyperparam}
\begin{tabular}{|l|c|c|}
\hline 
Method & Hyperparameter & Value \\ 
\hline \hline
\multirow{2}{*}{Trick Prior} & $C_1$ & $\dfrac{C}{N}\dfrac{1}{H + V}$ \\
 & $C_2$ & $\dfrac{C}{N}\dfrac{1}{H*V}$ \\
\hline
\multirow{2}{*}{Truncated Normal} & $\sigma^2_{main}$ & $\dfrac{1}{H + V}$ \\
 & $\sigma^2_{interaction}$ & $\dfrac{1}{H*V}$ \\
\hline
\multirow{2}{*}{Marginalized Likelihood} & $\sigma^2_{main}$ & $\dfrac{1}{H + V}$ \\
 & $\sigma^2_{interaction}$ & $\dfrac{1}{H*V}$ \\
\hline
\end{tabular}
\end{table}

# Mixing

The truncated Normal method (2) and the marginalized likelihood method (3) are drawing from the same stationary posterior distribution for images. 

```{r models-load}
load("data/sample_images.Rdata")
load("data/params_theta.Rdata")

params_good <- list(main_hidden = sample.params %>% ungroup() %>% filter(!near_hull) %>% select(starts_with("h"), -H) %>% data.matrix(),
                    main_visible = sample.params %>% ungroup() %>% filter(!near_hull) %>% select(starts_with("v"), -V) %>% data.matrix(),
                    interaction = sample.params %>% ungroup() %>% filter(!near_hull) %>% select(starts_with("theta")) %>% data.matrix() %>% matrix(4))

#data and params ------------------------
H <- 4
V <- 4

#marginalized likelihood
load("data/fitted_models_trunc_marginal_full.Rdata")
marginal_bad <- models_bad
marginal_good <- models_good

#load trick prior
load("data/fitted_models_jing_5.8.Rdata")
trick_bad <- models_bad
trick_good <- models_good

#truncated normal
load("data/fitted_models_trunc_full.Rdata")
trunc_bad <- models_bad
trunc_good <- models_good

#rm unneccesary data
rm(models_bad)
rm(models_good)

#computing actual distributions ---------------------
flat_images_good$visibles %>%
  data.frame() %>% 
  rename(v1 = X1, v2 = X2, v3 = X3, v4 = X4) %>%
  group_by(v1, v2, v3, v4) %>%
  summarise(prob = n()/nrow(flat_images_good$visibles)) -> distn_emp

distn_good <- visible_distn(params = params_good)

reshape_sample_distn <- function(model) {
  sample_distn <- model$distn
  dim(sample_distn) <- c(dim(sample_distn)[1]*dim(sample_distn)[2], dim(sample_distn)[3])
  sample_distn %>% data.frame() -> sample_distn
  names(sample_distn) <- names(distn_good)
  
  sample_distn %>%
    group_by(image_id) %>%
    mutate(iter = 1:n()) -> sample_distn
  
  return(sample_distn)
}

marginal_sample_good <- reshape_sample_distn(marginal_good)
trick_sample_good <- reshape_sample_distn(trick_good)
trunc_sample_good <- reshape_sample_distn(trunc_good)
```

```{r acf, fig.cap="\\label{fig:acf}The autocorrelation functions (ACF) for each image with the truncated Normal method in black and the marginalized likelihood method in red.", fig.height=5, fig.width=8, out.width=".7\\linewidth"}
marginal_sample_good %>% 
  filter(iter > 50) %>%
  group_by(v1, v2, v3, v4, image_id) %>% 
  do(data.frame(acf = acf(.$prob, plot=FALSE)$acf,
     lag = acf(.$prob, plot=FALSE)$lag)) -> marginal_acfs

trunc_sample_good %>% 
  filter(iter > 50) %>%
  group_by(v1, v2, v3, v4, image_id) %>% 
  do(data.frame(acf = acf(.$prob, plot=FALSE)$acf,
                lag = acf(.$prob, plot=FALSE)$lag)) -> trunc_acfs

marginal_acfs %>% ungroup() %>% select(-image_id) %>% rename(marginal = acf) %>%
  left_join(trunc_acfs %>% rename(truncated = acf)) %>%
  ggplot() +
  geom_segment(aes(x=lag, xend=lag, y=0, yend=truncated), colour = "black") +
  geom_segment(aes(x=lag, xend=lag, y=0, yend=marginal), colour = "red") +
  facet_wrap(~image_id) +
  xlab("Lag") +
  ylab("ACF")

```

```{r M_eff, cache=TRUE}
###
### Overlapping block means
###
overlap_mean <- function(data, b) {
  n <- length(data)
  N <- n - b + 1
  
  blockmeans <- rep(0, N)
  for(i in 1:N) {
    blockmeans[i] <- mean(data[i:(b + i - 1)])
  }
  
  blockmeans
}

marginal_sample_good %>% 
  group_by(v1, v2, v3, v4, image_id) %>%
  do(means = overlap_mean(.$prob, 2000^(1/3))) -> marginal_means

trunc_sample_good %>% 
  group_by(v1, v2, v3, v4, image_id) %>%
  do(means = overlap_mean(.$prob, 5000^(1/3))) -> trunc_means

marginal_means %>%
  group_by(v1, v2, v3, v4, image_id) %>%
  do(data.frame(C = 2000^(1/3)*var(.$means[[1]]))) %>%
  left_join(marginal_sample_good %>% group_by(v1, v2, v3, v4) %>% summarise(sigma2 = var(prob))) %>%
  mutate(M_eff = sigma2/C,
         model = "Marginal Likelihood") %>%
  bind_rows(trunc_means %>%
    group_by(v1, v2, v3, v4, image_id) %>%
    do(data.frame(C = 5000^(1/3)*var(.$means[[1]]))) %>%
    left_join(trunc_sample_good %>% group_by(v1, v2, v3, v4) %>% summarise(sigma2 = var(prob))) %>%
    mutate(M_eff = sigma2/C,
           model = "Truncated Normal")) %>%
  select(-C, -sigma2) %>%
  spread(model, M_eff) %>%
  ungroup() %>%
  select(-(v1:v4)) %>%
  gather(Model, `$M_{eff}$`, -image_id) %>%
  mutate(`$M_{eff}$` = 1000*`$M_{eff}$`) %>%
  spread(image_id, `$M_{eff}$`) -> M_eff_table

M_eff_ratio <- M_eff_table[1,-1]/M_eff_table[2,-1]
```

# Effective sample size

We can use an overlapping blockmeans approach to get a crude estimate for the aysmptotic variance of the probability of each image and compare it to an estimate of the asymptotic variance assuming IID draws from the target distribution. 

\tiny

```{r m_eff_table}
M_eff_table %>%
  gather(Image, M_eff, -Model) %>%
  spread(Model, M_eff) %>%
  mutate(Image = as.numeric(Image)) %>%
  arrange(Image) -> M_eff_tmp

cbind(M_eff_tmp[1:8,], M_eff_tmp[9:16,]) %>%
  kable(digits = 2, caption = paste0("\\label{tab:m_eff}The effective sample sizes for a chain of length $M = 1000$ of all $16$ images."))
```

# Posterior distributions of images
```{r posterior-dsn}
marginal_sample_good %>% rename(marginal = prob) %>% filter(iter > 50) %>%
  left_join(trick_sample_good %>% ungroup() %>% select(-image_id) %>% rename(trick = prob)) %>%
  ungroup() %>%
  mutate(image_id = paste0("image_", image_id)) %>% 
  gather(method, prob, trick, marginal) %>%
  spread(image_id, prob) -> all_statistics

all_statistics %>%
  gather(statistic, value, -iter, -method, -starts_with("v")) %>%
  filter(grepl("image", statistic) & !is.na(value)) %>%
  separate(statistic, into = c("junk", "statistic")) %>%
  mutate(statistic = factor(statistic, labels = paste("Image", 1:length(unique(statistic))))) %>%
  left_join(distn_emp %>% rename(prob_emp = prob) %>% right_join(distn_good %>% select(-image_id))) %>%
  ggplot() +
  geom_density(aes(value, y=..scaled.., colour = method, fill = method), alpha = .2) +
  geom_vline(aes(xintercept = prob)) +
  geom_vline(aes(xintercept = prob_emp), colour = "red") +
  facet_wrap(~statistic, scales = "free_x") + 
  ylab("Scaled Posterior Density") + xlab("Probability of Image") +
  scale_colour_discrete("Method", labels = c("Marginalized likelihood", "Trick prior")) +
  scale_fill_discrete("Method", labels = c("Marginalized likelihood", "Trick prior")) +
  theme(legend.position = "bottom") -> p.models
```

```{r fitting_plot, fig.cap="\\label{fig:fitting_plot}Posterior probability of each possible 4-pixel image using two of the three Bayesian fitting techniques, trick prior and marginalized likelihood. The black vertical lines show the true probabilities of each image.", fig.height=4.5}
p.models
```

# Wrapping up

- While RBMs can be useful for classification, in the context of a statistical model as a representation of data, RBMs are a poor fit due to
    1. *near-degeneracy*, 
    2. *instability*, and
    3. *uninterpretability*.
- Rigorous fitting methodology is possible but slow and as the size of the model grows, becomes intractible.
- There is no "smoothing" achieved with a RBM fit using a rigorous method, because any fully principled fitting method will reproduce the empirical distribution of most realistic training sets

---- 
\begin{center}
\Huge
Concliques $+$ Gibbs $= \text{ Cool}^3$
\end{center}

# Goal

- Markov random field models are common for spatial data
- Rather than specifying a joint distribution directly, a  model is specified through a set of full conditional distributions for each spatial location
- Assume the spatial data are on a regular lattice (wrapped on a torus)

**Goal:** A new, provably fast approach for simulating spatial data.

# Spatial Markov random field (MRF) models

\begin{block}{Notation}
\begin{itemize}
\item Variables  $\{ Y(\mbs_i): i=1, \dots, n \}$ at locations $\{ \mbs_i: i=1, \dots, n\}$
\item Neighborhoods: $N_i$ specified according to some configuration
\item Neighboring Values: $\mby(N_i) = \{ y(\mbs_j) : \mbs_j \in N_i \}$
\item Full Conditionals: $\{ f_i(y(\mbs_i)|\mby(N_i), \mbtheta): i=1, \dots, n \}$
\begin{itemize}
    \item $f_i(y(\mbs_i)|\mby(N_i), \mbtheta)$ is conditional pmf/pdf of $Y(\mbs_i)$ given values for its
 neighbors  $\mby(N_i)$
    \item Often assume a common conditional cdf $F_i=F$ form ($f_i=f$) for all $i$
\end{itemize}
\end{itemize}
\end{block}

# Exponential family examples

1. Conditional Gaussian (3 parameters):  
    $$
      f_i(y(\mbs_i)|\mby(N_i),\alpha,\eta,\tau) = \frac{1}{\sqrt{2 \pi} \tau}\exp\left( -\frac{[y(\mbs_i) - \mu(\mbs_i) ]^2}{2 \tau^2}\right)
    $$
    $Y(\mbs_i)$ given neighbors $\mby(N_i)$ is normal with variance $\tau^2$ and mean
    $$
    \mu(\mbs_i) = \alpha + \eta \sum_{\mbs_j \in N_i}[y(\mbs_j)-\alpha]
    $$
2. Conditional Binary (2 parameters):  
    $Y(\mbs_i)$ given neighbors $\mby(N_i)$ is Bernoulli $p(\mbs_i,\kappa,\eta)$ where
    $$
    \mathrm{logit}[p(\mbs_i,\kappa,\eta)] = \mathrm{logit}( \kappa ) +\eta \sum_{\mbs_j \in N_i}[y(\mbs_j)-\kappa]
    $$

In both examples, $\eta$ represents a dependence parameter.

# Concliques

\begin{block}{Cliques -- \cite{hammersley1971markov}}
Singletons and sets of locations such that each location in the set is a neighbor of all other locations in the set \\
Example: Four nearest neighbors gives cliques of sizes $1$ and $2$
\end{block}

\begin{block}{The Converse of Cliques -- Concliques}
Sets of locations such that no location in the set is a neighbor of any other location in the set

\vspace{-.5cm}

\footnotesize
\begin{columns}
\hspace*{-.8cm}
\begin{column}{0.1\textwidth}
\begin{center}
\vspace*{-.9cm}
\underline{4 Nearest}\\ \underline{Neighbors}
$$
\begin{array}{ccc}
\cdot&*&\cdot \\
*&\mbs&* \\
\cdot&*&\cdot \\
\end{array}
$$
\end{center}
\end{column}
\begin{column}{0.1\textwidth}
\begin{center}
 \underline{Concliques}\\ \underline{4 Nearest}\\ \underline{Neighbors}
$$
\begin{array}{cccc}
1&2&1&2 \\
2&1&2&1 \\
1&2&1&2 \\
2&1&2&1 \\
\end{array}
$$
\end{center}
\end{column}
\begin{column}{0.1\textwidth}
\begin{center}
\vspace*{-.9cm}
\underline{8 Nearest}\\ \underline{Neighbors}
$$
\begin{array}{ccc}
*&*&* \\
*&\mbs&* \\
*&*&* \\
\end{array}
$$
\end{center}
\end{column}
\begin{column}{0.1\textwidth}
\begin{center}

\vspace*{-.2cm}
 \underline{Concliques}\\ \underline{8 Nearest}\\ \underline{Neighbors}
$$
\begin{array}{cccc}
1&2&1&2 \\
3&4&3&4 \\
1&2&1&2 \\
3&4&3&4 \\
\end{array}
$$
\end{center}
\end{column}
\end{columns}
\end{block}

# Generalized spatial residuals

\begin{block}{Definition}
\begin{itemize}
%\item Let ${\cal R}$ be a spatial lattice on which variables are defined (may be irregular in shape)
\item   $F(y|\mby(N_i), \mbtheta)$ is the conditional cdf of $Y(\mbs_i)$ under the model
\item Substitute random variables, $Y(\mbs_i)$ and neighbors $\{Y(\mbs_j): \mbs_j \in N_i \}$, into  (continuous) conditional cdf  to define residuals:
\begin{displaymath}
R(\mbs_i) = F(Y(\mbs_i)|\{Y(\mbs_j): \mbs_j \in N_i \}, \mbtheta).
\end{displaymath}
\end{itemize}
\end{block}

\begin{block}{Key Property}
Let $\{ {\cal C}_j: \, j=1, \ldots, q \}$ be a collection of concliques that partition the integer grid. Under the conditional model, \textbf{spatial residuals {\it within} a conclique are iid  Uniform$(0, 1)$-distributed}: \\
\begin{displaymath}
\{ R(\mbs_i): \, \mbs_i \in {\cal C}_j \} \stackrel{iid}{\sim} \mbox{ Uniform}(0, 1) \qquad \text{ for } j=1, \dots, q
\end{displaymath}
\end{block}

[@kaiser2012goodness]

# Conclique-based Gibbs sampler

Using the conditional independence of random variables at locations within a conclique along with the probability integral transform we propose a conclique-based Gibbs sampling algorithm for sampling from a MRF.

1. Split locations into $Q$ disjoint concliques, $\mathcal{D} = \cup_{i = 1}^Q\mathcal{C}_i$.
1. Initialize the values of $\{Y^{(0)}(\boldsymbol s): \boldsymbol s \in \{\mathcal{C}_2, \dots, \mathcal{C}_Q\}\}$.
1. Sample from the conditional distribution of $Y(\boldsymbol s)$ given $\{Y(\boldsymbol t ) : \boldsymbol t \in \mathcal{N}(\boldsymbol s )\}$ for $\boldsymbol s \in \mathcal{C}_1$, 
    a. Sample $\{U(s): s \in \mathcal{C}_1\} \stackrel{iid}{\sim} Unif(0,1)$
    b. For each $\boldsymbol s \in \mathcal{C}_1$, $Y^{(i)}(\boldsymbol s) = F^{-1}(U(\boldsymbol s)|Y^{(i-1)}(\boldsymbol t), \boldsymbol t \in \mathcal{N}(\boldsymbol s))$
1. Sample from the conditional distribution of $Y(\boldsymbol s)$ given $\{Y(\boldsymbol t ) : \boldsymbol t \in \mathcal{N}(\boldsymbol s )\}$ for $\boldsymbol s \in \mathcal{C}_j; j =2, \dots, Q$, 
    a. Sample $\{U(s): s \in \mathcal{C}_2\} \stackrel{iid}{\sim} Unif(0,1)$
    b. For each $\boldsymbol s \in \mathcal{C}_j$, $Y^{(i)}(\boldsymbol s) = F^{-1}(U(\boldsymbol s)|\{Y^{(i)}(\boldsymbol t), \boldsymbol t \in \mathcal{N}(\boldsymbol s) \cap \mathcal{C}_k \text{ where } k < j\}, \{Y^{(i-1)}(\boldsymbol t), \boldsymbol t \in \mathcal{N}(\boldsymbol s) \cap \mathcal{C}_k \text{ where } k > j\})$

# It's (provably) fast!

1. In many (commonly used) four-nearest neighbor models (including Gaussian and binary), the conclique-based Gibbs sampler is **geometrically ergodic**.
1. Because we are using batch updating vs. sequential updating of each location, this approach is also **computationally fast**.
1. This approach is implemented in a flexible `R` package (called `conclique`, to appear on CRAN) that implements a conclique-based Gibbs sampler while allowing the user to specify an arbitrary model.


---- 
\begin{center}
\Huge
Future work
\end{center}

# Ideas

- Using AdaBoost to create an ensemble learner for combining classifiers
- Goodness-of-fit test for network data
    - The model-based method of resampling re-frames network into a collection of neighborhoods by using covariate information
    - Creates concliques on a graph structure.
    - Use a conditionally specified network distribution to sample network data in a blockwise conclique-based Gibbs sampler. 
- Non-parametric methods of resampling network data
    - Blockwise bootstrap, analogous to the blockwise bootstrap in time series, where network data blocks are created from the network by selecting a node and all nodes connected to it by an edge.
    - These blocks are then sampled with replacement and sewn together to re-create a total network by using iid Bernoulli draws (with an appropriate probability of success).
    - This method does not impose a neighborhood structure, nor particular distributional assumptions.

# Appendix: Proof of Result \ref{prop:instab}
\scriptsize

We prove the contrapositive. Suppose that $\Delta_V(\boldsymbol \theta) \le C$ holds for some $C > 0$. Under the RBM model for visibles, $P_{\boldsymbol \theta}(\boldsymbol v) > 0$ holds for each outcome $\boldsymbol v \in \{-1,1\}^V$. Let $\boldsymbol v_{min} \equiv \argmin\limits_{\boldsymbol v \in \{-1, 1\}^V}P_{\boldsymbol \theta}(\boldsymbol v)$ and $\boldsymbol v_{max} \equiv \argmax\limits_{\boldsymbol v \in \{-1, 1\}^V}P_{\boldsymbol \theta}(\boldsymbol v)$. Note there exists a sequence $\boldsymbol v_{min} \equiv \boldsymbol v_0, \boldsymbol v_1, \dots, \boldsymbol v_k \equiv \boldsymbol v_{max}$ in $\{-1,1\}^V$ of component-wise switches to move from $\boldsymbol v_{min}$ to $\boldsymbol v_{max}$ in the sample space (i.e. $\boldsymbol v_i, \boldsymbol v_{i + 1} \in \{-1,1\}^V$ differ by exactly $1$ component for $i = 0, \dots, k$) for some integer $k \in \{0, 1, \dots, V\}$. Then
\begin{align*}
\log\left[\frac{P_{\boldsymbol \theta}(\boldsymbol v_{max})}{P_{\boldsymbol \theta}(\boldsymbol v_{min})}\right] &= \left|\sum\limits_{i = 1}^k\log\left(\frac{P_{\boldsymbol \theta}(\boldsymbol v_i)}{P_{\boldsymbol \theta}(\boldsymbol v_{i-1})}\right)\right| \\
& \le \sum\limits_{i = 1}^k\left|\log\left(\frac{P_{\boldsymbol \theta}(\boldsymbol v_i)}{P_{\boldsymbol \theta}(\boldsymbol v_{i-1})}\right)\right| \\
& \le k \Delta_V(\boldsymbol \theta) \le VC
\end{align*}
using $k \le V$ and $\Delta_V(\boldsymbol \theta) \le C$.$\Box$

# Appendix: Proof of Result \ref{prop:degen}

\scriptsize 

Define $\boldsymbol v_{max}$ and $\boldsymbol v_{min}$ as in the proof of Proposition \ref{prop:instab}. Fix $0 < \epsilon < 1$ Then, $\boldsymbol v_{max} \in M_{\epsilon, \boldsymbol \theta}$, so $P_{\boldsymbol \theta}(M_{\epsilon, \boldsymbol \theta}) \ge P_{\boldsymbol \theta}(\boldsymbol v_{max})$. If $\boldsymbol v \in \{-1, 1\}^V \setminus M_{\epsilon, \boldsymbol \theta}$, then by definition $P_{\boldsymbol \theta}(\boldsymbol v) \le [P_{\boldsymbol \theta}(\boldsymbol v_{max})]^{1-\epsilon}[P_{\boldsymbol \theta}(\boldsymbol v_{min})]^{\epsilon}$ holds so that
\begin{align*}
1-P_{\boldsymbol \theta}(M_{\epsilon, \boldsymbol \theta}) &= P_{\boldsymbol \theta}(M_{\epsilon, \boldsymbol \theta}^C) \\
& = \sum\limits_{\boldsymbol v \in \{-1, 1\}^V \setminus M_{\epsilon, \boldsymbol \theta}}P_{\boldsymbol \theta}(\boldsymbol v) \\
& \le (2^V)[P_{\boldsymbol \theta}(\boldsymbol v_{max})]^{1-\epsilon}[P_{\boldsymbol \theta}(\boldsymbol v_{min})]^{\epsilon}
\end{align*}
Then,
\begin{align*}
\frac{1}{V}\log\left[\frac{P_{\boldsymbol \theta}(M_{\epsilon, \boldsymbol \theta})}{1-P_{\boldsymbol \theta}(M_{\epsilon, \boldsymbol \theta})}\right] & \ge \frac{1}{V} \log\left[\frac{P_{\boldsymbol\theta}(\boldsymbol v_{max})}{(2^V)[P_{\boldsymbol \theta}(\boldsymbol v_{max})]^{1-\epsilon}[P_{\boldsymbol \theta}(\boldsymbol v_{min})]^{\epsilon}}\right] \\
&= \frac{\epsilon}{V} \log\left[\frac{P_{\boldsymbol \theta}(\boldsymbol v_{max})}{P_{\boldsymbol \theta}(\boldsymbol v_{min})}\right] - \log 2 \rightarrow \infty
\end{align*}
as $V \rightarrow \infty$ by the definition of an unstable RBM model.$\Box$

# Appendix: Proof of Equation (\ref{eqn:lowerbnd})

\tiny

\begin{align*}
\log\left[\frac{\max\limits_{\boldsymbol v \in \{-1, 1\}^V}P_{\boldsymbol \theta}\boldsymbol v}{\min\limits_{\boldsymbol v \in \{-1, 1\}^V}P_{\boldsymbol \theta}\boldsymbol v} \right]  &= \log\left[\frac{\max\limits_{\boldsymbol v}\sum\limits_{\boldsymbol h \in \{-1,1\}^H} \exp\left(\sum\limits_{i = 1}^V \theta_{v_i}v_i + \sum\limits_{j = 1}^H \theta_{h_j}h_j + \sum\limits_{i = 1}^V\sum\limits_{j = 1}^H \theta_{ij}v_i h_j\right)}
{\min\limits_{\boldsymbol v}\sum\limits_{\boldsymbol h \in \{-1,1\}^H} \exp\left(\sum\limits_{i = 1}^V \theta_{v_i}v_i + \sum\limits_{j = 1}^H \theta_{h_j}h_j + \sum\limits_{i = 1}^V\sum\limits_{j = 1}^H \theta_{ij}v_i h_j\right)}\right] \\
& \ge \log\frac{\max\limits_{\boldsymbol v}\max\limits_{\boldsymbol h \in \{-1,1\}^H}\exp\left(\sum\limits_{i = 1}^V \theta_{v_i}v_i + \sum\limits_{j = 1}^H \theta_{h_j}h_j + \sum\limits_{i = 1}^V\sum\limits_{j = 1}^H \theta_{ij}v_i h_j\right)}
{\min\limits_{\boldsymbol v}2^H\max\limits_{\boldsymbol h \in \{-1,1\}^H}\exp\left(\sum\limits_{i = 1}^V \theta_{v_i}v_i + \sum\limits_{j = 1}^H \theta_{h_j}h_j + \sum\limits_{i = 1}^V\sum\limits_{j = 1}^H \theta_{ij}v_i h_j\right)} \\
&  = \max\limits_{\boldsymbol v}\max\limits_{\boldsymbol h \in \{-1,1\}^H}\left\{\sum\limits_{i = 1}^V \theta_{v_i}v_i + \sum\limits_{j = 1}^H \theta_{h_j}h_j + \sum\limits_{i = 1}^V\sum\limits_{j = 1}^H \theta_{ij}v_i h_j\right\} - \\
&  \qquad \min\limits_{\boldsymbol v}\max\limits_{\boldsymbol h \in \{-1,1\}^H}\left\{\sum\limits_{i = 1}^V \theta_{v_i}v_i + \sum\limits_{j = 1}^H \theta_{h_j}h_j + \sum\limits_{i = 1}^V\sum\limits_{j = 1}^H \theta_{ij}v_i h_j\right\} -H\log2 \\
&  \equiv R(\boldsymbol \theta)
\end{align*}
using above the monotonicity of $\log(\cdot)$.

# Appendix: Parameters used

\scriptsize

```{r param-table}
sample.params %>% 
  ungroup %>% 
  filter(!near_hull) %>% select(starts_with("v"), starts_with("h"), starts_with("theta"), -H, -V) %>%
  rename(`$\\theta_{v1}$` = v1,
         `$\\theta_{v2}$` = v2,
         `$\\theta_{v3}$` = v3,
         `$\\theta_{v4}$` = v4,
         `$\\theta_{h1}$` = h1,
         `$\\theta_{h2}$` = h2,
         `$\\theta_{h3}$` = h3,
         `$\\theta_{h4}$` = h4,
         `$\\theta_{11}$` = theta11,
         `$\\theta_{12}$` = theta12,
         `$\\theta_{13}$` = theta13,
         `$\\theta_{14}$` = theta14,
         `$\\theta_{21}$` = theta21,
         `$\\theta_{22}$` = theta22,
         `$\\theta_{23}$` = theta23,
         `$\\theta_{24}$` = theta24,
         `$\\theta_{31}$` = theta31,
         `$\\theta_{32}$` = theta32,
         `$\\theta_{33}$` = theta33,
         `$\\theta_{34}$` = theta34,
         `$\\theta_{41}$` = theta41,
         `$\\theta_{42}$` = theta42,
         `$\\theta_{43}$` = theta43,
         `$\\theta_{44}$` = theta44) %>%
  gather("Parameter", "Value") -> tbl

cbind(tbl[1:8,], tbl[9:16,], tbl[17:24,]) %>%
  kable(caption = "\\label{tab:theta}Parameters used to fit a test case with $V = H = 4$. This parameter vector was chosen as a sampled value of $\\boldsymbol \\theta$ that was not near the convex hull of the sufficient statistics for a grid point in figure \\ref{fig:degen_plots} with $< 5$\\% near-degeneracy.")
```

# References
